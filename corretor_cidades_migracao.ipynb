{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importando a lista de treino, com os nomes corretos das cidades.**"
      ],
      "metadata": {
        "id": "EMoqfs23HUpF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqx6niFlGGBK",
        "outputId": "fcd9bbf3-bfba-4767-e5be-2a4975308323"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acrelândia\n",
            "Assis Brasil\n",
            "Brasiléia\n",
            "Bujari\n",
            "Capixaba\n",
            "Cruzeiro do Sul\n",
            "Epitaciolândia\n",
            "Feijó\n",
            "Jordão\n",
            "Mâncio Lima\n",
            "Manoel Urbano\n",
            "Marechal Thaumaturgo\n",
            "Plácido de Castro\n",
            "Porto Acre\n",
            "Porto Walter\n",
            "Rio Branco\n",
            "Rodrigues Alves\n",
            "Santa Rosa do Purus\n",
            "Senador Guiomard\n",
            "Sena Madureira\n",
            "Tarauacá\n",
            "Xapuri\n",
            "Água Branca\n",
            "Anadia\n",
            "Arapiraca\n",
            "Atalaia\n",
            "Barra de Santo Antônio\n",
            "Barra de São Miguel\n",
            "Batalha\n",
            "Belém\n",
            "Belo Monte\n",
            "Boca da Mata\n",
            "Branquinha\n",
            "Cacimbinhas\n",
            "Cajueiro\n",
            "Campestre\n",
            "Campo Alegre\n",
            "Campo Grande\n",
            "Canapi\n",
            "Capela\n",
            "Carneiros\n",
            "Chã Preta\n",
            "Co\n"
          ]
        }
      ],
      "source": [
        "with open('lista_cidades_treino.txt', 'r') as f:\n",
        "  lista_treino = f.read()\n",
        "\n",
        "print(lista_treino[:500])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importando o módulo nltk (natural lenguage toolkit)\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgUAWWo5GMcI",
        "outputId": "97f04936-9279-4eb4-c52f-3a311e8edb15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# separando as cidades em tokens (tokenização)\n",
        "\n",
        "lista_tokens = nltk.tokenize.word_tokenize(lista_treino)\n",
        "print(f'O número de palavras é {len(lista_tokens)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAgcgaJ0H0zP",
        "outputId": "6053153c-ad32-4914-ad56-6a486a5a6d5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O número de palavras é 10288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# agora vamos contar quantas palavras diferentes temos na lista\n",
        "# antes de contar as palavras diferentes vamos normalizar as palavras para lower case\n",
        "\n",
        "def normalizacao(lista_palavras):\n",
        "  lista_normalizada = []\n",
        "  for palavra in lista_palavras:\n",
        "    lista_normalizada.append(palavra.lower())\n",
        "  return lista_normalizada"
      ],
      "metadata": {
        "id": "QW9RAhe0I7ze"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista_normalizada = normalizacao(lista_tokens)\n",
        "print(lista_normalizada[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcE6gn6tKhjc",
        "outputId": "e3ef7ccc-f907-4126-c416-82a343319b2b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['acrelândia', 'assis', 'brasil', 'brasiléia', 'bujari']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos remover as palavras repetidas criando o set\n",
        "# para então contar com len\n",
        "\n",
        "len(set(lista_normalizada))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKD2ZRSOKqA0",
        "outputId": "eac2a5b7-fcb7-4b85-c7b8-b2e60778abd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3971"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# desenvolvendo o corretor para inserir letra que deixou de ser digitada\n",
        "# a técnica será dividir a palavra em lados esquerdo e direito\n",
        "# e, após, inserir a letra faltante entre esses dois lados\n",
        "# se caso for a última letra faltante, na função gerador_palavras será inserida\n",
        "\n",
        "def insere_letras(fatias):\n",
        "  novas_palavras= []\n",
        "  letras = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'\n",
        "  for E, D in fatias:\n",
        "    for letra in letras:\n",
        "      novas_palavras.append(E + letra + D)\n",
        "  return novas_palavras\n",
        "\n",
        "# agora vamos incrementar o corretor para corrigir letras a mais na palavra (lóigica)\n",
        "# função que deleta sempre o index zero da fatia da direita\n",
        "\n",
        "def deletando_caracteres(fatias):\n",
        "  novas_palavras = []\n",
        "  for E, D in fatias:\n",
        "    novas_palavras.append(E + D[1:])\n",
        "  return novas_palavras\n",
        "\n",
        "# vamos incrementar nossa solução para corrigir trocas de letras nas palavras (lígica)\n",
        "\n",
        "def troca_letras(fatias):\n",
        "  novas_palavras = []\n",
        "  letras = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'\n",
        "  for E, D in fatias:\n",
        "    for letra in letras:\n",
        "      novas_palavras.append(E + letra + D[1:])\n",
        "  return novas_palavras\n",
        "\n",
        "# incrementando o algoritmo para corrigir inversão de letras (lgóica)\n",
        "def inverte_letras(fatias):\n",
        "  novas_palavras = []\n",
        "  for E, D in fatias:\n",
        "    if len(D) > 1:\n",
        "      novas_palavras.append(E + D[1] + D[0] + D[2:])\n",
        "  return novas_palavras\n",
        "\n",
        "def gerador_palavras(palavra):\n",
        "  fatias = []\n",
        "  letras = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'\n",
        "  for i in range(len(palavra)):\n",
        "    fatias.append((palavra[:i],palavra[i:]))\n",
        "  palavras_geradas = insere_letras(fatias)\n",
        "  for letra in letras:\n",
        "    palavras_geradas.append(palavra + letra)\n",
        "  palavras_geradas += deletando_caracteres(fatias)\n",
        "  palavras_geradas += troca_letras(fatias)\n",
        "  palavras_geradas += inverte_letras(fatias)\n",
        "  return palavras_geradas\n",
        "\n"
      ],
      "metadata": {
        "id": "woCw5CHCK0ay"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# como calcular a probabilidade = frequencia_palavra/total_de_palavras\n",
        "# primeiro passo encontrar a frequencia das palavras\n",
        "# no nltk tem um método que conta isso\n",
        "\n",
        "frequencia = nltk.FreqDist(lista_normalizada)\n",
        "total_palavras = len(lista_normalizada)\n",
        "frequencia.most_common(10)\n",
        "\n",
        "def probabilidade(palavra_gerada):\n",
        "  return frequencia[palavra_gerada]/total_palavras\n",
        "\n",
        "# função que verifica as palavras geradas e seleciona a correta\n",
        "# dentro da função max passamos no parametro key uma função que calculará a probabilidade\n",
        "# das palavras, retornando a com maior probabilidade de ser a correta\n",
        "\n",
        "def corretor(palavra):\n",
        "  palavra_norm = palavra.lower()\n",
        "  palavras_corretas = []\n",
        "  if \" \" in palavra:\n",
        "    palavras = palavra_norm.split()\n",
        "    for p in palavras:\n",
        "      palavras_geradas = gerador_palavras(p)\n",
        "      palavra_correta = max(palavras_geradas, key=probabilidade)\n",
        "      palavras_corretas.append(palavra_correta.capitalize())\n",
        "      palavras_corrigidas = \" \".join(palavras_corretas)\n",
        "    return palavras_corrigidas\n",
        "  else:\n",
        "    palavras_geradas = gerador_palavras(palavra_norm)\n",
        "    palavra_corrigida = max(palavras_geradas, key=probabilidade)\n",
        "    return palavra_corrigida.capitalize()\n",
        "\n",
        "corretor(\"gRAVATA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fUzGkk5w3FrR",
        "outputId": "b53753b0-8a91-4a07-e441-b82b20a94253"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Gravatal'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1t47D3XlZzFd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}